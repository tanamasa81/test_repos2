

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>RaNNC 0.4.2.post13 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html#document-index" class="icon icon-home" alt="Documentation Home"> RaNNC
          

          
          </a>

          
            
            
          

          

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
            
              <!-- Local TOC -->
              <div class="local-toc"><p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-tutorial">Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-limitations">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-faq">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-references">API References</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-logging">Logging</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-build">Building from source</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-config">Configurations</a></li>
</ul>
</div>
            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html#document-index">RaNNC</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html#document-index" class="icon icon-home"></a> &raquo;</li>
        
      <li>RaNNC 0.4.2.post13 documentation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="rannc-rapid-neural-network-connector">
<h1>RaNNC (Rapid Neural Network Connector)<a class="headerlink" href="#rannc-rapid-neural-network-connector" title="Permalink to this headline">¶</a></h1>
<p>RaNNC is a deep learning framework for automatic model/data parallelism.
RaNNC decomposes a computational graph of a PyTorch model and distributes the subgraphs onto multiple compute nodes.
You can train a billion-scale parameter models using RaNNC.</p>
<blockquote class="epigraph">
<div><p>Automatic Graph Partitioning for Very Large-scale Deep Learning, Masahiro Tanaka, Kenjiro Taura, Toshihiro Hanawa and Kentaro Torisawa, In the Proceedings of 35th IEEE International Parallel and Distributed Processing Symposium (IPDPS 2021), Portland, Oregon USA, May, 2021. (to appear)</p>
</div></blockquote>
<div class="toctree-wrapper compound">
<span id="document-installation"></span><div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h3>
<p>RaNNC works only with CUDA devices (CPU only/TPU environments are not supported).
RaNNC requires following libraries and tools at runtime.</p>
<ul class="simple">
<li><p><em>CUDA</em>: A CUDA runtime must be available at the runtime environment. Currently RaNNC is tested with CUDA 10.2.</p></li>
<li><p><em>NCCL</em>: NCCL (Version &gt;= 2.7.3 is required) must be available at the runtime environment. RaNNC uses NCCL both for allreduce and P2P communications.</p></li>
<li><p><em>MPI</em>: A program using RaNNC must be launched with MPI. MPI libraries must also be available at runtime. RaNNC is tested with OpenMPI v4.0.5.</p></li>
<li><p><em>libstd++</em>: <code class="docutils literal notranslate"><span class="pre">libstd++</span></code> must support <code class="docutils literal notranslate"><span class="pre">GLIBCXX_3.4.21</span></code> to use the distributed <code class="docutils literal notranslate"><span class="pre">pip</span></code> packages (The packages are built with gcc 5.4.0).</p></li>
</ul>
</div>
<div class="section" id="id1">
<h3>Installation<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>This version of RaNNC requires PyTorch v1.7.1.
<code class="docutils literal notranslate"><span class="pre">pip</span></code> packages for <code class="docutils literal notranslate"><span class="pre">linux_x86_64</span></code> are available from the following links.</p>
<ul class="simple">
<li><p><code class="xref download docutils literal notranslate"><span class="pre">For</span> <span class="pre">Python</span> <span class="pre">3.7</span></code></p></li>
<li><p><code class="xref download docutils literal notranslate"><span class="pre">For</span> <span class="pre">Python</span> <span class="pre">3.8</span></code></p></li>
</ul>
<p>You can create a new conda environment and install RaNNC by the following commands.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda create -n rannc <span class="nv">python</span><span class="o">=</span><span class="m">3</span>.8
conda activate rannc
conda install <span class="nv">pytorch</span><span class="o">==</span><span class="m">1</span>.7.1 <span class="nv">cudatoolkit</span><span class="o">=</span><span class="m">10</span>.2 -c pytorch
pip install pyrannc-0.5-cp38-cp38m-linux_x86_64.whl
</pre></div>
</div>
</div>
</div>
<span id="document-tutorial"></span><div class="section" id="tutorial">
<h2>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h2>
<p>RaNNC distributes PyTorch models onto multiple nodes or processes using MPI.
Follow the steps below to learn the basic usage of RaNNC.</p>
<div class="section" id="steps-to-use-rannc">
<h3>Steps to use RaNNC<a class="headerlink" href="#steps-to-use-rannc" title="Permalink to this headline">¶</a></h3>
<div class="section" id="set-up-environment">
<h4>0. Set up environment<a class="headerlink" href="#set-up-environment" title="Permalink to this headline">¶</a></h4>
<p>Ensure required tools and libraries (CUDA, NCCL, OpenMPI, etc.) are available.
The libraries must be included in <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> at runtime.</p>
</div>
<div class="section" id="import-rannc">
<h4>1. Import RaNNC<a class="headerlink" href="#import-rannc" title="Permalink to this headline">¶</a></h4>
<p>Insert <code class="docutils literal notranslate"><span class="pre">import</span></code> in your script.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyrannc</span>
</pre></div>
</div>
</div>
<div class="section" id="wrap-your-model">
<h4>2. Wrap your model<a class="headerlink" href="#wrap-your-model" title="Permalink to this headline">¶</a></h4>
<p>Wrap your model by <code class="docutils literal notranslate"><span class="pre">pyrannc.RaNNCModule</span></code> with your optimizer.
You can use the wrapped model in almost same manner as the original model (See below).
Note that the original model must be on a CUDA device.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">pyrannc</span><span class="o">.</span><span class="n">RaNNCModule</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
</pre></div>
</div>
<p>If you don’t use an optimizer, pass only the model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">pyrannc</span><span class="o">.</span><span class="n">RaNNCModule</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="run-forward-backward-passes">
<h4>3. Run forward/backward passes<a class="headerlink" href="#run-forward-backward-passes" title="Permalink to this headline">¶</a></h4>
<p>A <code class="docutils literal notranslate"><span class="pre">RaNNCModule</span></code> can run forward/backward passes as with a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
</pre></div>
</div>
<p>Inputs to <code class="docutils literal notranslate"><span class="pre">RaNNCModule</span></code> must be CUDA tensors.
RaNNCModule has several more limitations about a wrapped model and inputs/outputs.
See <a class="reference internal" href="index.html#document-limitations"><span class="doc">Limitations</span></a> for details.
The optimizer can update model parameters just by calling <code class="docutils literal notranslate"><span class="pre">step()</span></code>.</p>
<p>The program below (<code class="docutils literal notranslate"><span class="pre">examples/tutorial_usage.py</span></code>) shows the above usage with a very simple model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="kn">as</span> <span class="nn">optim</span>

<span class="kn">import</span> <span class="nn">pyrannc</span>


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">pyrannc</span><span class="o">.</span><span class="n">RaNNCModule</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">opt</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">))</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

<span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="launch">
<h4>4. Launch<a class="headerlink" href="#launch" title="Permalink to this headline">¶</a></h4>
<p>A program using RaNNC requires to be launched by <code class="docutils literal notranslate"><span class="pre">mpirun</span></code>.
You can launch the above example script by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun -np <span class="m">2</span> python tutorial_usage.py
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">-np</span></code> indicates the number of ranks (processes).
RaNNC allocates one CUDA device for each rank.
In the above example, there must be two available CUDA devices.</p>
</div>
</div>
<div class="section" id="how-rannc-works">
<h3>How RaNNC works<a class="headerlink" href="#how-rannc-works" title="Permalink to this headline">¶</a></h3>
<div class="section" id="automatic-parallelism">
<h4>Automatic parallelism<a class="headerlink" href="#automatic-parallelism" title="Permalink to this headline">¶</a></h4>
<p>RaNNC analyzes a given model and determines the best combination of different parallelisms.
RaNNC can combine the three following parallelisms.</p>
<ul class="simple">
<li><p>Data parallelism</p></li>
<li><p>Model parallelism</p></li>
<li><p>Pipeline parallelism</p></li>
</ul>
<p>See <a class="reference external" href="http://...">IPDPS 2021 paper</a> for the details of the automatic parallelism.</p>
</div>
<div class="section" id="data-distribution">
<h4>Data distribution<a class="headerlink" href="#data-distribution" title="Permalink to this headline">¶</a></h4>
<p>Each process launched by MPI is expected to load different (mini-)batches. RaNNC automatically gathers the batches from all ranks and compute them as one batch.
<code class="docutils literal notranslate"><span class="pre">torch.utils.data.distributed.DistributedSampler</span></code> will be useful for this purpose.</p>
</div>
</div>
</div>
<span id="document-limitations"></span><div class="section" id="limitations">
<h2>Limitations<a class="headerlink" href="#limitations" title="Permalink to this headline">¶</a></h2>
<p>Although a <code class="docutils literal notranslate"><span class="pre">RaNNCModel</span></code> is designed to work like <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>, it has the following limitations.</p>
<div class="section" id="control-constructs-are-ignored">
<h3>Control constructs are ignored<a class="headerlink" href="#control-constructs-are-ignored" title="Permalink to this headline">¶</a></h3>
<p>RaNNC uses a computation graph produced by PyTorch’s <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html">tracing function</a>.
As explained in the document, the tracing function does not record control constructs including conditional branches and loops.</p>
</div>
<div class="section" id="arguments-and-return-values">
<h3>Arguments and return values<a class="headerlink" href="#arguments-and-return-values" title="Permalink to this headline">¶</a></h3>
<p>Arguments and outputs of a <code class="docutils literal notranslate"><span class="pre">RaNNCModel</span></code> must satisfy the following conditions.</p>
<ul class="simple">
<li><p>Arguments must be (mini-)batches tensors, whose first dimension corresponds to samples in a mini-batch.</p></li>
<li><p>Keyword arguments are not allowed.</p></li>
<li><p>Outputs must be (mini-)batches tensors, or a loss value (scalar tensor).</p></li>
</ul>
</div>
<div class="section" id="tensor-data-types">
<h3>Tensor data types<a class="headerlink" href="#tensor-data-types" title="Permalink to this headline">¶</a></h3>
<p>Currently RaNNC does not support <em>TF32 (TensorFloat-32)</em>.</p>
</div>
</div>
<span id="document-faq"></span><div class="section" id="faqs">
<h2>FAQs<a class="headerlink" href="#faqs" title="Permalink to this headline">¶</a></h2>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#does-rannc-work-with-apex-amp" id="id1">Does RaNNC work with Apex AMP?</a></p></li>
<li><p><a class="reference internal" href="#how-to-save-load-a-rannc-module" id="id2">How to save/load a RaNNC module</a></p></li>
<li><p><a class="reference internal" href="#how-to-use-gradient-accumulation" id="id3">How to use gradient accumulation</a></p></li>
<li><p><a class="reference internal" href="#my-model-takes-too-long-until-partitioning-is-determined" id="id4">My model takes too long until partitioning is determined</a></p></li>
<li><p><a class="reference internal" href="#custom-cpp-functions-does-not-work-with-rannc" id="id5">Custom cpp functions does not work with RaNNC</a></p></li>
<li><p><a class="reference internal" href="#how-to-use-a-model-that-takes-kwargs" id="id6">How to use a model that takes kwargs</a></p></li>
<li><p><a class="reference internal" href="#does-rannc-work-with-torch-distributed-package" id="id7">Does RaNNC work with torch.distributed package?</a></p></li>
</ul>
</div>
<div class="section" id="does-rannc-work-with-apex-amp">
<h3><a class="toc-backref" href="#id1">Does RaNNC work with Apex AMP?</a><a class="headerlink" href="#does-rannc-work-with-apex-amp" title="Permalink to this headline">¶</a></h3>
<p>Yes.
Convert your model with <code class="docutils literal notranslate"><span class="pre">amp.initialize()</span></code> and pass
the resulting model to <code class="docutils literal notranslate"><span class="pre">RaNNCModule</span></code> with <code class="docutils literal notranslate"><span class="pre">use_amp_master_params=True</span></code>.</p>
</div>
<div class="section" id="how-to-save-load-a-rannc-module">
<h3><a class="toc-backref" href="#id2">How to save/load a RaNNC module</a><a class="headerlink" href="#how-to-save-load-a-rannc-module" title="Permalink to this headline">¶</a></h3>
<p>Use <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> of the RaNNC module.
The returned <em>state_dict</em> can be saved and loaded as with PyTorch.</p>
<p>Please make sure <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> must be called from all ranks.
Otherwise, the call of <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> is blocked because RaNNC gathers parameters across all ranks.</p>
</div>
<div class="section" id="how-to-use-gradient-accumulation">
<h3><a class="toc-backref" href="#id3">How to use gradient accumulation</a><a class="headerlink" href="#how-to-use-gradient-accumulation" title="Permalink to this headline">¶</a></h3>
<p>As default, RaNNC implicitly performs allreduce (sum) of gradients on all ranks after a backward pass.
To prevent the allreduce, you can use <code class="docutils literal notranslate"><span class="pre">pyrannc.delay_grad_allreduce(False)</span></code>.</p>
<p>After a specified number of forward/backward steps, you can explicitly perform allreduce
with <code class="docutils literal notranslate"><span class="pre">allreduce_grads</span></code> of your <code class="docutils literal notranslate"><span class="pre">RaNNCModule</span></code>.</p>
</div>
<div class="section" id="my-model-takes-too-long-until-partitioning-is-determined">
<h3><a class="toc-backref" href="#id4">My model takes too long until partitioning is determined</a><a class="headerlink" href="#my-model-takes-too-long-until-partitioning-is-determined" title="Permalink to this headline">¶</a></h3>
<p>By setting <code class="docutils literal notranslate"><span class="pre">save_deployment=true</span></code>, RaNNC outputs the deployment state to a file <code class="docutils literal notranslate"><span class="pre">deployment_file</span></code> after
partitioning is determined. You can load the deployment file by setting <code class="docutils literal notranslate"><span class="pre">load_deployment=true</span></code>.
This greatly save your time if you run a program using RaNNC with similar settings, e.g. with different learning rate.
(See also <a class="reference internal" href="index.html#document-config"><span class="doc">Configurations</span></a>)</p>
<p>When you are unsure that partitioning process keeps going or already failed, you can change the log level of
the partitioning module. Changing log levels of <code class="docutils literal notranslate"><span class="pre">MLPartitioner</span></code> and <code class="docutils literal notranslate"><span class="pre">DPStaging</span></code> will show you the progress of
partitioning process.
(See also <a class="reference internal" href="index.html#document-logging"><span class="doc">Logging</span></a>)</p>
</div>
<div class="section" id="custom-cpp-functions-does-not-work-with-rannc">
<h3><a class="toc-backref" href="#id5">Custom cpp functions does not work with RaNNC</a><a class="headerlink" href="#custom-cpp-functions-does-not-work-with-rannc" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="how-to-use-a-model-that-takes-kwargs">
<h3><a class="toc-backref" href="#id6">How to use a model that takes kwargs</a><a class="headerlink" href="#how-to-use-a-model-that-takes-kwargs" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="does-rannc-work-with-torch-distributed-package">
<h3><a class="toc-backref" href="#id7">Does RaNNC work with torch.distributed package?</a><a class="headerlink" href="#does-rannc-work-with-torch-distributed-package" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<span id="document-references"></span><div class="section" id="api-references">
<h2>API References<a class="headerlink" href="#api-references" title="Permalink to this headline">¶</a></h2>
</div>
<span id="document-logging"></span><div class="section" id="logging">
<h2>Logging<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h2>
<p>RaNNC uses <a class="reference external" href="https://github.com/gabime/spdlog">spdlog</a> and
<a class="reference external" href="https://github.com/guangie88/spdlog_setup">spdlog_setup</a> for logging.
You can configure the logging by a configuration file
placed at <code class="docutils literal notranslate"><span class="pre">~/.pyrannc/logging.toml</span></code>.</p>
<p>Since RaNNC has loggers associated with internal modules,
you can set a log level for each module.
The below shows an example of the loggign configuration file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">global_pattern</span> <span class="o">=</span> <span class="s2">&quot;[%Y-%m-</span><span class="si">%d</span><span class="s2"> %T.</span><span class="si">%f</span><span class="s2">] [%L] &lt;%n&gt;: %v&quot;</span>

<span class="c1"># Sinks</span>
<span class="p">[[</span><span class="n">sink</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;console_st&quot;</span>
<span class="nb">type</span> <span class="o">=</span> <span class="s2">&quot;stdout_sink_st&quot;</span>

<span class="p">[[</span><span class="n">sink</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;stderr_st&quot;</span>
<span class="nb">type</span> <span class="o">=</span> <span class="s2">&quot;color_stdout_sink_st&quot;</span>

<span class="c1"># Loggers</span>
<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;root&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;console_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RaNNCModule&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RaNNCProcess&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;GraphLauncher&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;GraphValueStorage&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;GraphUtil&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Decomposer&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Decomposition&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;GraphProfiler&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ParamStorage&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;GraphConnector&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;TorchDriver&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;AllReduceRunner&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;MLPartitioner&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>

<span class="p">[[</span><span class="n">logger</span><span class="p">]]</span>
<span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;DPStaging&quot;</span>
<span class="n">sinks</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;stderr_st&quot;</span><span class="p">]</span>
<span class="n">level</span> <span class="o">=</span> <span class="s2">&quot;info&quot;</span>
</pre></div>
</div>
</div>
<span id="document-build"></span><div class="section" id="building-from-source">
<h2>Building from source<a class="headerlink" href="#building-from-source" title="Permalink to this headline">¶</a></h2>
<div class="section" id="compiler-version">
<h3>Compiler version<a class="headerlink" href="#compiler-version" title="Permalink to this headline">¶</a></h3>
<p>You must use GCC v5.4 or newer. We tested RaNNC with GCC v5.4 and v7.1.
Note that, however, RaNNC must be built complying ABI of PyTorch.</p>
<p>RaNNC is built with <em>Pre-cxx11 ABI</em> (<code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>) as default because PyTorch installed via conda is built with <em>Pre-cxx11 ABI</em>.
You can change the ABI setting in <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code>.
PyTorch provides you with a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compiled_with_cxx11_abi.html">function</a> below to know how the binary is compiled.</p>
</div>
<div class="section" id="build-and-install">
<h3>Build and Install<a class="headerlink" href="#build-and-install" title="Permalink to this headline">¶</a></h3>
<p>You need to set some environment variables before building RaNNC to help cmake find dependent libraries.</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">Variables for building configurations</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Variable</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>CUDA_HOME</p></td>
<td><p>Path to a CUDA runtime directory.</p></td>
</tr>
<tr class="row-odd"><td><p>MPI_DIR</p></td>
<td><p>Path to an MPI installation directory.</p></td>
</tr>
<tr class="row-even"><td><p>BOOST_DIR</p></td>
<td><p>Path to a Boost libraries directory.</p></td>
</tr>
<tr class="row-odd"><td><p>CUDNN_ROOT_DIR</p></td>
<td><p>Path to a cuDNN libraries directory.</p></td>
</tr>
<tr class="row-even"><td><p>LD_LIBRARY_PATH</p></td>
<td><p>Must contain the path to NCCL lib directory.</p></td>
</tr>
</tbody>
</table>
<p>The building process refers to PyTorch installed with conda.
Therefore, install PyTorch with your python and run <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>.
The following script shows configurations to install RaNNC from the source.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env bash</span>

 <span class="c1"># Activate conda</span>
 <span class="nb">source</span> <span class="o">[</span>CONDA_PATH<span class="o">]</span>/etc/profile.d/conda.sh
 conda activate rannc

 <span class="c1"># Set dependencies</span>
 <span class="nb">export</span> <span class="nv">CUDA_HOME</span><span class="o">=</span><span class="s2">&quot;</span><span class="k">$(</span>dirname <span class="k">$(</span>which nvcc<span class="k">))</span><span class="s2">/../&quot;</span>
 <span class="nb">export</span> <span class="nv">MPI_DIR</span><span class="o">=</span><span class="s2">&quot;</span><span class="k">$(</span>dirname <span class="k">$(</span>which ompi_info<span class="k">))</span><span class="s2">/../&quot;</span>
 <span class="nb">export</span> <span class="nv">BOOST_DIR</span><span class="o">=[</span>BOOST_DIR_PATH<span class="o">]</span>
 <span class="nb">export</span> <span class="nv">CUDNN_ROOT_DIR</span><span class="o">=[</span>YOUR_CUDNN_DIR_PATH<span class="o">]</span>

 python setup.py build -g install
</pre></div>
</div>
<p><em>Makefiles</em> under <code class="docutils literal notranslate"><span class="pre">docker/</span></code> show the complete process to build and install RaNNC.
They are used to build pip packages.</p>
</div>
</div>
<span id="document-config"></span><div class="section" id="configurations">
<h2>Configurations<a class="headerlink" href="#configurations" title="Permalink to this headline">¶</a></h2>
<p>RaNNC’s runtime configurations can be set in the following two ways:</p>
<ul class="simple">
<li><p><em>Config file</em>: RaNNC automatically loads a configuration file at <code class="docutils literal notranslate"><span class="pre">~/.pyrannc/rannc_conf.toml</span></code>. Names of configuration items must be in lower case. The path to the configuration file can be set by an environment variable <code class="docutils literal notranslate"><span class="pre">RANNC_CONF_DIR</span></code>.</p></li>
<li><p><em>Environment variables</em>: You can overwrite configuration by setting environment variables. Names of variables follows <code class="docutils literal notranslate"><span class="pre">RANNC_&lt;CONF_ITEM_NAME&gt;</span></code> in upper case. For example, you can set <cite>mem_margin</cite> in the following table by a variable <code class="docutils literal notranslate"><span class="pre">RANNC_MEM_MARGIN</span></code>.</p></li>
</ul>
<table class="colwidths-given docutils align-default" id="id1">
<caption><span class="caption-text">Configurations</span><a class="headerlink" href="#id1" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 10%" />
<col style="width: 70%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Default</p></th>
<th class="head"></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>mem_margin</p></td>
<td><p>0.1</p></td>
<td><p>Memory margin for model partitioning.</p></td>
</tr>
<tr class="row-odd"><td><p>save_deployment</p></td>
<td><p>true</p></td>
<td><p>Save deployment of a partitioned model if set to true.</p></td>
</tr>
<tr class="row-even"><td><p>load_deployment</p></td>
<td><p>false</p></td>
<td><p>Load deployment of a partitioned model if set to true.</p></td>
</tr>
<tr class="row-odd"><td><p>deployment_file</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/tmp/rannc_deployment.bin</span></code></p></td>
<td><p>Path of deployment file to save/load.</p></td>
</tr>
<tr class="row-even"><td><p>min_pipeline</p></td>
<td><p>1</p></td>
<td><p>Minimum number of microbatches for pipeline parallelism</p></td>
</tr>
<tr class="row-odd"><td><p>max_pipeline</p></td>
<td><p>32</p></td>
<td><p>Maxmum number of microbatches for pipeline parallelism</p></td>
</tr>
<tr class="row-even"><td><p>opt_param_factor</p></td>
<td><p>2</p></td>
<td><p>Factor to estimate memory usage by an optimizer. For example, Set this item to 2 for Adam because the optimizer uses two internal data <cite>v</cite> and <cite>s</cite>, whose sizes are equivalent to parameter tensors.</p></td>
</tr>
<tr class="row-odd"><td><p>trace_events</p></td>
<td><p>false</p></td>
<td><p>Trace internal events if set to true. When true, the event tracing significantly degrades performance.</p></td>
</tr>
<tr class="row-even"><td><p>event_trace_file</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">/tmp/rannc_event_trace.json</span></code></p></td>
<td><p>Path to an event trace file.</p></td>
</tr>
</tbody>
</table>
<p>The following is an example of the configuration file.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">profiling</span><span class="o">=</span><span class="n">false</span>
<span class="n">dump_graph</span><span class="o">=</span><span class="n">false</span>
<span class="n">dump_graph_prefix</span><span class="o">=</span><span class="s2">&quot;graph_dump&quot;</span>
<span class="n">partition_num</span><span class="o">=</span><span class="mi">2</span>
<span class="n">replica_num</span><span class="o">=</span><span class="mi">1</span>
<span class="n">pipeline_num</span><span class="o">=</span><span class="mi">1</span>
<span class="n">validate_comm</span><span class="o">=</span><span class="n">false</span>
<span class="n">display_comm_value</span><span class="o">=</span><span class="n">false</span>
<span class="n">profiling_iter</span><span class="o">=</span><span class="mi">1</span>
<span class="n">consolidate_grads</span><span class="o">=</span><span class="n">false</span>
<span class="n">checkpointing</span><span class="o">=</span><span class="n">true</span>
<span class="n">checkpointing_no_last</span><span class="o">=</span><span class="n">false</span>
<span class="n">auto_parallel</span><span class="o">=</span><span class="n">false</span>
<span class="n">p2p_comm</span><span class="o">=</span><span class="n">true</span>
<span class="n">opt_param_factor</span><span class="o">=</span><span class="mi">2</span>
<span class="n">min_partition_num</span><span class="o">=</span><span class="mi">5</span>
<span class="n">max_partition_num</span><span class="o">=</span><span class="mi">30</span>
<span class="n">mem_margin</span><span class="o">=</span><span class="mf">0.1</span>
<span class="n">do_uncoarsening</span><span class="o">=</span><span class="n">true</span>
<span class="n">min_pipeline</span><span class="o">=</span><span class="mi">1</span>
<span class="n">max_pipeline</span><span class="o">=</span><span class="mi">4</span>
<span class="n">save_deployment</span><span class="o">=</span><span class="n">true</span>
<span class="n">load_deployment</span><span class="o">=</span><span class="n">false</span>
<span class="n">save_graph_profile</span><span class="o">=</span><span class="n">false</span>
<span class="n">load_graph_profile</span><span class="o">=</span><span class="n">false</span>
<span class="n">trace_events</span><span class="o">=</span><span class="n">false</span>
<span class="n">verify_recomp</span><span class="o">=</span><span class="n">false</span>
<span class="n">coarsen_by_time</span><span class="o">=</span><span class="n">false</span>
<span class="n">skip_grad_scaling</span><span class="o">=</span><span class="n">false</span>
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, DIRECT, National Institute of Information and Communications Technology

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>