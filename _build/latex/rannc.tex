%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}

% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}
\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{0}



\title{RaNNC}
\date{Jan 21, 2021}
\release{0.4.2.post13}
\author{DIRECT, National Institute of Information and Communications Technology}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}


RaNNC is a deep learning framework for automatic model/data parallelism.
RaNNC decomposes a computational graph of a PyTorch model and distributes the subgraphs onto multiple compute nodes.
You can train a billion-scale parameter models using RaNNC.
\begin{quote}

Automatic Graph Partitioning for Very Large-scale Deep Learning, Masahiro Tanaka, Kenjiro Taura, Toshihiro Hanawa and Kentaro Torisawa, In the Proceedings of 35th IEEE International Parallel and Distributed Processing Symposium (IPDPS 2021), Portland, Oregon USA, May, 2021. (to appear)
\end{quote}


\chapter{Installation}
\label{\detokenize{installation:installation}}\label{\detokenize{installation::doc}}

\section{Prerequisites}
\label{\detokenize{installation:prerequisites}}
RaNNC works only with CUDA devices (CPU only/TPU environments are not supported).
RaNNC requires following libraries and tools at runtime.
\begin{itemize}
\item {} 
\sphinxstyleemphasis{CUDA}: A CUDA runtime must be available at the runtime environment. Currently RaNNC is tested with CUDA 10.2.

\item {} 
\sphinxstyleemphasis{NCCL}: NCCL (Version \textgreater{}= 2.7.3 is required) must be available at the runtime environment. RaNNC uses NCCL both for allreduce and P2P communications.

\item {} 
\sphinxstyleemphasis{MPI}: A program using RaNNC must be launched with MPI. MPI libraries must also be available at runtime. RaNNC is tested with OpenMPI v4.0.5.

\item {} 
\sphinxstyleemphasis{libstd++}: \sphinxcode{\sphinxupquote{libstd++}} must support \sphinxcode{\sphinxupquote{GLIBCXX\_3.4.21}} to use the distributed \sphinxcode{\sphinxupquote{pip}} packages (The packages are built with gcc 5.4.0).

\end{itemize}


\section{Installation}
\label{\detokenize{installation:id1}}
This version of RaNNC requires PyTorch v1.7.1.
\sphinxcode{\sphinxupquote{pip}} packages for \sphinxcode{\sphinxupquote{linux\_x86\_64}} are available from the following links.
\begin{itemize}
\item {} 
\sphinxcode{\sphinxupquote{For Python 3.7}}

\item {} 
\sphinxcode{\sphinxupquote{For Python 3.8}}

\end{itemize}

You can create a new conda environment and install RaNNC by the following commands.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
conda create \PYGZhy{}n rannc \PYG{n+nv}{python}\PYG{o}{=}\PYG{l+m}{3}.8
conda activate rannc
conda install \PYG{n+nv}{pytorch}\PYG{o}{=}\PYG{o}{=}\PYG{l+m}{1}.7.1 \PYG{n+nv}{cudatoolkit}\PYG{o}{=}\PYG{l+m}{10}.2 \PYGZhy{}c pytorch
pip install pyrannc\PYGZhy{}0.5\PYGZhy{}cp38\PYGZhy{}cp38m\PYGZhy{}linux\PYGZus{}x86\PYGZus{}64.whl
\end{sphinxVerbatim}


\chapter{Tutorial}
\label{\detokenize{tutorial:tutorial}}\label{\detokenize{tutorial::doc}}
RaNNC distributes PyTorch models onto multiple nodes or processes using MPI.
Follow the steps below to learn the basic usage of RaNNC.


\section{Steps to use RaNNC}
\label{\detokenize{tutorial:steps-to-use-rannc}}

\subsection{0. Set up environment}
\label{\detokenize{tutorial:set-up-environment}}
Ensure required tools and libraries (CUDA, NCCL, OpenMPI, etc.) are available.
The libraries must be included in \sphinxcode{\sphinxupquote{LD\_LIBRARY\_PATH}} at runtime.


\subsection{1. Import RaNNC}
\label{\detokenize{tutorial:import-rannc}}
Insert \sphinxcode{\sphinxupquote{import}} in your script.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pyrannc}
\end{sphinxVerbatim}


\subsection{2. Wrap your model}
\label{\detokenize{tutorial:wrap-your-model}}
Wrap your model by \sphinxcode{\sphinxupquote{pyrannc.RaNNCModule}} with your optimizer.
You can use the wrapped model in almost same manner as the original model (See below).
Note that the original model must be on a CUDA device.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{Net}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cuda}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{opt} \PYG{o}{=} \PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{pyrannc}\PYG{o}{.}\PYG{n}{RaNNCModule}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{optimizer}\PYG{p}{)}
\end{sphinxVerbatim}

If you don’t use an optimizer, pass only the model.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{pyrannc}\PYG{o}{.}\PYG{n}{RaNNCModule}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{3. Run forward/backward passes}
\label{\detokenize{tutorial:run-forward-backward-passes}}
A \sphinxcode{\sphinxupquote{RaNNCModule}} can run forward/backward passes as with a \sphinxcode{\sphinxupquote{torch.nn.Module}}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cuda}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{out} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
\PYG{n}{out}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn\PYGZus{}like}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

Inputs to \sphinxcode{\sphinxupquote{RaNNCModule}} must be CUDA tensors.
RaNNCModule has several more limitations about a wrapped model and inputs/outputs.
See {\hyperref[\detokenize{limitations::doc}]{\sphinxcrossref{\DUrole{doc}{Limitations}}}} for details.
The optimizer can update model parameters just by calling \sphinxcode{\sphinxupquote{step()}}.

The program below (\sphinxcode{\sphinxupquote{examples/tutorial\_usage.py}}) shows the above usage with a very simple model.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{import} \PYG{n+nn}{torch.nn} \PYG{k+kn}{as} \PYG{n+nn}{nn}
\PYG{k+kn}{import} \PYG{n+nn}{torch.optim} \PYG{k+kn}{as} \PYG{n+nn}{optim}

\PYG{k+kn}{import} \PYG{n+nn}{pyrannc}


\PYG{k}{class} \PYG{n+nc}{Net}\PYG{p}{(}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{Net}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fc1} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Linear}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{bias}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fc2} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Linear}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{bias}\PYG{o}{=}\PYG{n+nb+bp}{False}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fc1}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fc2}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{x}


\PYG{n}{model} \PYG{o}{=} \PYG{n}{Net}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cuda}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{opt} \PYG{o}{=} \PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{pyrannc}\PYG{o}{.}\PYG{n}{RaNNCModule}\PYG{p}{(}\PYG{n}{model}\PYG{p}{,} \PYG{n}{opt}\PYG{p}{)}

\PYG{n}{x} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{n+nb+bp}{True}\PYG{p}{)}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{device}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cuda}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{out} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}

\PYG{n}{target} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn\PYGZus{}like}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
\PYG{n}{out}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{n}{target}\PYG{p}{)}

\PYG{n}{opt}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}


\subsection{4. Launch}
\label{\detokenize{tutorial:launch}}
A program using RaNNC requires to be launched by \sphinxcode{\sphinxupquote{mpirun}}.
You can launch the above example script by:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
mpirun \PYGZhy{}np \PYG{l+m}{2} python tutorial\PYGZus{}usage.py
\end{sphinxVerbatim}

\sphinxcode{\sphinxupquote{-np}} indicates the number of ranks (processes).
RaNNC allocates one CUDA device for each rank.
In the above example, there must be two available CUDA devices.


\section{How RaNNC works}
\label{\detokenize{tutorial:how-rannc-works}}

\subsection{Automatic parallelism}
\label{\detokenize{tutorial:automatic-parallelism}}
RaNNC analyzes a given model and determines the best combination of different parallelisms.
RaNNC can combine the three following parallelisms.
\begin{itemize}
\item {} 
Data parallelism

\item {} 
Model parallelism

\item {} 
Pipeline parallelism

\end{itemize}

See \sphinxhref{http://...}{IPDPS 2021 paper} for the details of the automatic parallelism.


\subsection{Data distribution}
\label{\detokenize{tutorial:data-distribution}}
Each process launched by MPI is expected to load different (mini-)batches. RaNNC automatically gathers the batches from all ranks and compute them as one batch.
\sphinxcode{\sphinxupquote{torch.utils.data.distributed.DistributedSampler}} will be useful for this purpose.


\chapter{Limitations}
\label{\detokenize{limitations:limitations}}\label{\detokenize{limitations::doc}}
Although a \sphinxcode{\sphinxupquote{RaNNCModel}} is designed to work like \sphinxcode{\sphinxupquote{torch.nn.Module}}, it has the following limitations.


\section{Control constructs are ignored}
\label{\detokenize{limitations:control-constructs-are-ignored}}
RaNNC uses a computation graph produced by PyTorch’s \sphinxhref{https://pytorch.org/docs/stable/generated/torch.jit.trace.html}{tracing function}.
As explained in the document, the tracing function does not record control constructs including conditional branches and loops.


\section{Arguments and return values}
\label{\detokenize{limitations:arguments-and-return-values}}
Arguments and outputs of a \sphinxcode{\sphinxupquote{RaNNCModel}} must satisfy the following conditions.
\begin{itemize}
\item {} 
Arguments must be (mini-)batches tensors, whose first dimension corresponds to samples in a mini-batch.

\item {} 
Keyword arguments are not allowed.

\item {} 
Outputs must be (mini-)batches tensors, or a loss value (scalar tensor).

\end{itemize}


\section{Tensor data types}
\label{\detokenize{limitations:tensor-data-types}}
Currently RaNNC does not support \sphinxstyleemphasis{TF32 (TensorFloat-32)}.


\chapter{FAQs}
\label{\detokenize{faq:faqs}}\label{\detokenize{faq::doc}}
\begin{sphinxShadowBox}
\begin{itemize}
\item {} 
\phantomsection\label{\detokenize{faq:id1}}{\hyperref[\detokenize{faq:does-rannc-work-with-apex-amp}]{\sphinxcrossref{Does RaNNC work with Apex AMP?}}}

\item {} 
\phantomsection\label{\detokenize{faq:id2}}{\hyperref[\detokenize{faq:how-to-save-load-a-rannc-module}]{\sphinxcrossref{How to save/load a RaNNC module}}}

\item {} 
\phantomsection\label{\detokenize{faq:id3}}{\hyperref[\detokenize{faq:how-to-use-gradient-accumulation}]{\sphinxcrossref{How to use gradient accumulation}}}

\item {} 
\phantomsection\label{\detokenize{faq:id4}}{\hyperref[\detokenize{faq:my-model-takes-too-long-until-partitioning-is-determined}]{\sphinxcrossref{My model takes too long until partitioning is determined}}}

\item {} 
\phantomsection\label{\detokenize{faq:id5}}{\hyperref[\detokenize{faq:custom-cpp-functions-does-not-work-with-rannc}]{\sphinxcrossref{Custom cpp functions does not work with RaNNC}}}

\item {} 
\phantomsection\label{\detokenize{faq:id6}}{\hyperref[\detokenize{faq:how-to-use-a-model-that-takes-kwargs}]{\sphinxcrossref{How to use a model that takes kwargs}}}

\item {} 
\phantomsection\label{\detokenize{faq:id7}}{\hyperref[\detokenize{faq:does-rannc-work-with-torch-distributed-package}]{\sphinxcrossref{Does RaNNC work with torch.distributed package?}}}

\end{itemize}
\end{sphinxShadowBox}


\section{Does RaNNC work with Apex AMP?}
\label{\detokenize{faq:does-rannc-work-with-apex-amp}}
Yes.
Convert your model with \sphinxcode{\sphinxupquote{amp.initialize()}} and pass
the resulting model to \sphinxcode{\sphinxupquote{RaNNCModule}} with \sphinxcode{\sphinxupquote{use\_amp\_master\_params=True}}.


\section{How to save/load a RaNNC module}
\label{\detokenize{faq:how-to-save-load-a-rannc-module}}
Use \sphinxcode{\sphinxupquote{state\_dict()}} of the RaNNC module.
The returned \sphinxstyleemphasis{state\_dict} can be saved and loaded as with PyTorch.

Please make sure \sphinxcode{\sphinxupquote{state\_dict()}} must be called from all ranks.
Otherwise, the call of \sphinxcode{\sphinxupquote{state\_dict()}} is blocked because RaNNC gathers parameters across all ranks.


\section{How to use gradient accumulation}
\label{\detokenize{faq:how-to-use-gradient-accumulation}}
As default, RaNNC implicitly performs allreduce (sum) of gradients on all ranks after a backward pass.
To prevent the allreduce, you can use \sphinxcode{\sphinxupquote{pyrannc.delay\_grad\_allreduce(False)}}.

After a specified number of forward/backward steps, you can explicitly perform allreduce
with \sphinxcode{\sphinxupquote{allreduce\_grads}} of your \sphinxcode{\sphinxupquote{RaNNCModule}}.


\section{My model takes too long until partitioning is determined}
\label{\detokenize{faq:my-model-takes-too-long-until-partitioning-is-determined}}
By setting \sphinxcode{\sphinxupquote{save\_deployment=true}}, RaNNC outputs the deployment state to a file \sphinxcode{\sphinxupquote{deployment\_file}} after
partitioning is determined. You can load the deployment file by setting \sphinxcode{\sphinxupquote{load\_deployment=true}}.
This greatly save your time if you run a program using RaNNC with similar settings, e.g. with different learning rate.
(See also {\hyperref[\detokenize{config::doc}]{\sphinxcrossref{\DUrole{doc}{Configurations}}}})

When you are unsure that partitioning process keeps going or already failed, you can change the log level of
the partitioning module. Changing log levels of \sphinxcode{\sphinxupquote{MLPartitioner}} and \sphinxcode{\sphinxupquote{DPStaging}} will show you the progress of
partitioning process.
(See also {\hyperref[\detokenize{logging::doc}]{\sphinxcrossref{\DUrole{doc}{Logging}}}})


\section{Custom cpp functions does not work with RaNNC}
\label{\detokenize{faq:custom-cpp-functions-does-not-work-with-rannc}}

\section{How to use a model that takes kwargs}
\label{\detokenize{faq:how-to-use-a-model-that-takes-kwargs}}

\section{Does RaNNC work with torch.distributed package?}
\label{\detokenize{faq:does-rannc-work-with-torch-distributed-package}}

\chapter{API References}
\label{\detokenize{references:api-references}}\label{\detokenize{references::doc}}

\chapter{Logging}
\label{\detokenize{logging:logging}}\label{\detokenize{logging::doc}}
RaNNC uses \sphinxhref{https://github.com/gabime/spdlog}{spdlog} and
\sphinxhref{https://github.com/guangie88/spdlog\_setup}{spdlog\_setup} for logging.
You can configure the logging by a configuration file
placed at \sphinxcode{\sphinxupquote{\textasciitilde{}/.pyrannc/logging.toml}}.

Since RaNNC has loggers associated with internal modules,
you can set a log level for each module.
The below shows an example of the loggign configuration file.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{global\PYGZus{}pattern} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{Y\PYGZhy{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{m\PYGZhy{}}\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{T.}\PYG{l+s+si}{\PYGZpc{}f}\PYG{l+s+s2}{] [}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{L] \PYGZlt{}}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{n\PYGZgt{}: }\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{v}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Sinks}
\PYG{p}{[}\PYG{p}{[}\PYG{n}{sink}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{console\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n+nb}{type} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stdout\PYGZus{}sink\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{sink}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n+nb}{type} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{color\PYGZus{}stdout\PYGZus{}sink\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{c+c1}{\PYGZsh{} Loggers}
\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{root}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{console\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RaNNCModule}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RaNNCProcess}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GraphLauncher}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GraphValueStorage}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GraphUtil}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Decomposer}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Decomposition}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GraphProfiler}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ParamStorage}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{GraphConnector}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{TorchDriver}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AllReduceRunner}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{MLPartitioner}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}

\PYG{p}{[}\PYG{p}{[}\PYG{n}{logger}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{DPStaging}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{sinks} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{stderr\PYGZus{}st}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{level} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{info}\PYG{l+s+s2}{\PYGZdq{}}
\end{sphinxVerbatim}


\chapter{Building from source}
\label{\detokenize{build:building-from-source}}\label{\detokenize{build::doc}}

\section{Compiler version}
\label{\detokenize{build:compiler-version}}
You must use GCC v5.4 or newer. We tested RaNNC with GCC v5.4 and v7.1.
Note that, however, RaNNC must be built complying ABI of PyTorch.

RaNNC is built with \sphinxstyleemphasis{Pre-cxx11 ABI} (\sphinxcode{\sphinxupquote{\_GLIBCXX\_USE\_CXX11\_ABI=0}}) as default because PyTorch installed via conda is built with \sphinxstyleemphasis{Pre-cxx11 ABI}.
You can change the ABI setting in \sphinxcode{\sphinxupquote{CMakeLists.txt}}.
PyTorch provides you with a \sphinxhref{https://pytorch.org/docs/stable/generated/torch.compiled\_with\_cxx11\_abi.html}{function} below to know how the binary is compiled.


\section{Build and Install}
\label{\detokenize{build:build-and-install}}
You need to set some environment variables before building RaNNC to help cmake find dependent libraries.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Variables for building configurations}\label{\detokenize{build:id1}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|}
\hline

Variable
&\\
\hline
CUDA\_HOME
&
Path to a CUDA runtime directory.
\\
\hline
MPI\_DIR
&
Path to an MPI installation directory.
\\
\hline
BOOST\_DIR
&
Path to a Boost libraries directory.
\\
\hline
CUDNN\_ROOT\_DIR
&
Path to a cuDNN libraries directory.
\\
\hline
LD\_LIBRARY\_PATH
&
Must contain the path to NCCL lib directory.
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

The building process refers to PyTorch installed with conda.
Therefore, install PyTorch with your python and run \sphinxcode{\sphinxupquote{setup.py}}.
The following script shows configurations to install RaNNC from the source.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+ch}{\PYGZsh{}!/usr/bin/env bash}

 \PYG{c+c1}{\PYGZsh{} Activate conda}
 \PYG{n+nb}{source} \PYG{o}{[}CONDA\PYGZus{}PATH\PYG{o}{]}/etc/profile.d/conda.sh
 conda activate rannc

 \PYG{c+c1}{\PYGZsh{} Set dependencies}
 \PYG{n+nb}{export} \PYG{n+nv}{CUDA\PYGZus{}HOME}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{k}{\PYGZdl{}(}dirname \PYG{k}{\PYGZdl{}(}which nvcc\PYG{k}{)}\PYG{k}{)}\PYG{l+s+s2}{/../}\PYG{l+s+s2}{\PYGZdq{}}
 \PYG{n+nb}{export} \PYG{n+nv}{MPI\PYGZus{}DIR}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{k}{\PYGZdl{}(}dirname \PYG{k}{\PYGZdl{}(}which ompi\PYGZus{}info\PYG{k}{)}\PYG{k}{)}\PYG{l+s+s2}{/../}\PYG{l+s+s2}{\PYGZdq{}}
 \PYG{n+nb}{export} \PYG{n+nv}{BOOST\PYGZus{}DIR}\PYG{o}{=}\PYG{o}{[}BOOST\PYGZus{}DIR\PYGZus{}PATH\PYG{o}{]}
 \PYG{n+nb}{export} \PYG{n+nv}{CUDNN\PYGZus{}ROOT\PYGZus{}DIR}\PYG{o}{=}\PYG{o}{[}YOUR\PYGZus{}CUDNN\PYGZus{}DIR\PYGZus{}PATH\PYG{o}{]}

 python setup.py build \PYGZhy{}g install
\end{sphinxVerbatim}

\sphinxstyleemphasis{Makefiles} under \sphinxcode{\sphinxupquote{docker/}} show the complete process to build and install RaNNC.
They are used to build pip packages.


\chapter{Configurations}
\label{\detokenize{config:configurations}}\label{\detokenize{config::doc}}
RaNNC’s runtime configurations can be set in the following two ways:
\begin{itemize}
\item {} 
\sphinxstyleemphasis{Config file}: RaNNC automatically loads a configuration file at \sphinxcode{\sphinxupquote{\textasciitilde{}/.pyrannc/rannc\_conf.toml}}. Names of configuration items must be in lower case. The path to the configuration file can be set by an environment variable \sphinxcode{\sphinxupquote{RANNC\_CONF\_DIR}}.

\item {} 
\sphinxstyleemphasis{Environment variables}: You can overwrite configuration by setting environment variables. Names of variables follows \sphinxcode{\sphinxupquote{RANNC\_\textless{}CONF\_ITEM\_NAME\textgreater{}}} in upper case. For example, you can set \sphinxtitleref{mem\_margin} in the following table by a variable \sphinxcode{\sphinxupquote{RANNC\_MEM\_MARGIN}}.

\end{itemize}


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Configurations}\label{\detokenize{config:id1}}
\sphinxaftertopcaption
\begin{tabular}[t]{|\X{20}{100}|\X{10}{100}|\X{70}{100}|}
\hline
\sphinxstyletheadfamily 
Name
&\sphinxstyletheadfamily 
Default
&\sphinxstyletheadfamily \\
\hline
mem\_margin
&
0.1
&
Memory margin for model partitioning.
\\
\hline
save\_deployment
&
true
&
Save deployment of a partitioned model if set to true.
\\
\hline
load\_deployment
&
false
&
Load deployment of a partitioned model if set to true.
\\
\hline
deployment\_file
&
\sphinxcode{\sphinxupquote{/tmp/rannc\_deployment.bin}}
&
Path of deployment file to save/load.
\\
\hline
min\_pipeline
&
1
&
Minimum number of microbatches for pipeline parallelism
\\
\hline
max\_pipeline
&
32
&
Maxmum number of microbatches for pipeline parallelism
\\
\hline
opt\_param\_factor
&
2
&
Factor to estimate memory usage by an optimizer. For example, Set this item to 2 for Adam because the optimizer uses two internal data \sphinxtitleref{v} and \sphinxtitleref{s}, whose sizes are equivalent to parameter tensors.
\\
\hline
trace\_events
&
false
&
Trace internal events if set to true. When true, the event tracing significantly degrades performance.
\\
\hline
event\_trace\_file
&
\sphinxcode{\sphinxupquote{/tmp/rannc\_event\_trace.json}}
&
Path to an event trace file.
\\
\hline
\end{tabular}
\par
\sphinxattableend\end{savenotes}

The following is an example of the configuration file.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{profiling}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{dump\PYGZus{}graph}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{dump\PYGZus{}graph\PYGZus{}prefix}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{graph\PYGZus{}dump}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{n}{partition\PYGZus{}num}\PYG{o}{=}\PYG{l+m+mi}{2}
\PYG{n}{replica\PYGZus{}num}\PYG{o}{=}\PYG{l+m+mi}{1}
\PYG{n}{pipeline\PYGZus{}num}\PYG{o}{=}\PYG{l+m+mi}{1}
\PYG{n}{validate\PYGZus{}comm}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{display\PYGZus{}comm\PYGZus{}value}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{profiling\PYGZus{}iter}\PYG{o}{=}\PYG{l+m+mi}{1}
\PYG{n}{consolidate\PYGZus{}grads}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{checkpointing}\PYG{o}{=}\PYG{n}{true}
\PYG{n}{checkpointing\PYGZus{}no\PYGZus{}last}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{auto\PYGZus{}parallel}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{p2p\PYGZus{}comm}\PYG{o}{=}\PYG{n}{true}
\PYG{n}{opt\PYGZus{}param\PYGZus{}factor}\PYG{o}{=}\PYG{l+m+mi}{2}
\PYG{n}{min\PYGZus{}partition\PYGZus{}num}\PYG{o}{=}\PYG{l+m+mi}{5}
\PYG{n}{max\PYGZus{}partition\PYGZus{}num}\PYG{o}{=}\PYG{l+m+mi}{30}
\PYG{n}{mem\PYGZus{}margin}\PYG{o}{=}\PYG{l+m+mf}{0.1}
\PYG{n}{do\PYGZus{}uncoarsening}\PYG{o}{=}\PYG{n}{true}
\PYG{n}{min\PYGZus{}pipeline}\PYG{o}{=}\PYG{l+m+mi}{1}
\PYG{n}{max\PYGZus{}pipeline}\PYG{o}{=}\PYG{l+m+mi}{4}
\PYG{n}{save\PYGZus{}deployment}\PYG{o}{=}\PYG{n}{true}
\PYG{n}{load\PYGZus{}deployment}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{save\PYGZus{}graph\PYGZus{}profile}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{load\PYGZus{}graph\PYGZus{}profile}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{trace\PYGZus{}events}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{verify\PYGZus{}recomp}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{coarsen\PYGZus{}by\PYGZus{}time}\PYG{o}{=}\PYG{n}{false}
\PYG{n}{skip\PYGZus{}grad\PYGZus{}scaling}\PYG{o}{=}\PYG{n}{false}
\end{sphinxVerbatim}



\renewcommand{\indexname}{Index}
\printindex
\end{document}